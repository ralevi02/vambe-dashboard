# Vambe Dashboard
Dashboard de reuniones de venta con anÃ¡lisis de leads potenciado por IA, construido con Next.js 15.

**ðŸ”— Live app:** [https://vambe-dashboard-ten.vercel.app](https://vambe-dashboard-ten.vercel.app)  
**ðŸ“¦ Repositorio:** [https://github.com/ralevi02/vambe-dashboard](https://github.com/ralevi02/vambe-dashboard)

---

## Ejecutar localmente

### Requisitos previos

- Una API key de [Groq](https://console.groq.com) (gratuita)
- **Docker** (opciÃ³n recomendada) o Node.js 18+

---

### OpciÃ³n A â€” Docker Compose (recomendada)

```bash
# 1. Clonar el repositorio
git clone https://github.com/ralevi02/vambe-dashboard.git
cd vambe-dashboard

# 2. Crear el archivo de entorno
echo "GROQ_API_KEY=tu_clave_aqui" > .env

# 3. Construir y levantar
docker compose up --build
```

Abrir [http://localhost:3000](http://localhost:3000) en el navegador.  
Para detener: `docker compose down`

> El `Dockerfile` usa un build multi-stage (deps â†’ builder â†’ runner) y corre como usuario no-root para producciÃ³n.

---

### OpciÃ³n B â€” npm (desarrollo local)

```bash
# 1. Clonar el repositorio
git clone https://github.com/ralevi02/vambe-dashboard.git
cd vambe-dashboard

# 2. Instalar dependencias
npm install

# 3. Configurar variable de entorno
cp .env.example .env.local
# Editar .env.local y completar GROQ_API_KEY con tu clave

# 4. Iniciar servidor de desarrollo
npm run dev
```

Abrir [http://localhost:3000](http://localhost:3000) en el navegador.

---

### Variables de entorno

| Variable | DescripciÃ³n |
|---|---|
| `GROQ_API_KEY` | API key de Groq para el anÃ¡lisis con LLM |

> Docker Compose lee el archivo `.env`; el servidor de desarrollo de Next.js lee `.env.local`.

---

## Arquitectura

```
app/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ analyze-all/route.ts   # Endpoint SSE â€” streama resultados del anÃ¡lisis al cliente
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/                    # Primitivos reutilizables (Card, Badge, Modal, etc.)
â”‚   â”œâ”€â”€ clients/               # ClientsTable, ClientRow, ClientModal
â”‚   â”œâ”€â”€ sellers/               # Tabla y modal de vendedores
â”‚   â”œâ”€â”€ insights/              # GrÃ¡ficos Recharts (PainPoints, Channel, Sentiment)
â”‚   â”œâ”€â”€ dashboard/             # StatsRow, layout del dashboard principal
â”‚   â””â”€â”€ layout/                # Sidebar, ThemeToggle
â”œâ”€â”€ clients/                   # PÃ¡gina /clients
â”œâ”€â”€ sellers/                   # PÃ¡gina /sellers
â”œâ”€â”€ insights/                  # PÃ¡gina /insights
â”œâ”€â”€ settings/                  # PÃ¡gina /settings
â””â”€â”€ globals.css                # Design tokens CSS + temas claro/oscuro
lib/
â”œâ”€â”€ types.ts                   # Tipos TypeScript compartidos
â”œâ”€â”€ aiModel.ts                 # ComunicaciÃ³n con Groq, normalizaciÃ³n de salida del LLM
â”œâ”€â”€ insights.ts                # FunciÃ³n pura buildInsights() â€” agrega datos para grÃ¡ficos
â””â”€â”€ storage.ts                 # AbstracciÃ³n sobre localStorage
```

### Flujo de anÃ¡lisis IA

1. El usuario importa clientes via CSV o los carga desde `localStorage`.
2. Al hacer click en "Analizar con IA", el cliente llama a `GET /api/analyze-all`.
3. El endpoint divide los clientes en **batches de 10** y los envÃ­a al LLM **en paralelo** con `Promise.all` (ver secciÃ³n debajo).
4. Una vez que todos los batches responden, los resultados se streaman al cliente con **SSE** (Server-Sent Events) â€” el usuario ve las filas actualizarse progresivamente.
5. Los resultados se persisten en `localStorage` para evitar re-anÃ¡lisis innecesarios.

---

## Decisiones clave

### ParalelizaciÃ³n de batches para reducir tiempos de anÃ¡lisis

Sin paralelizar, analizar N clientes en batches secuenciales tomarÃ­a:

```
tiempo_total â‰ˆ batches Ã— latencia_LLM
```

Con `Promise.all`, todos los batches se despachan simultÃ¡neamente al LLM:

```
tiempo_total â‰ˆ latencia_del_batch_mÃ¡s_lento
```

Por ejemplo, con 50 clientes divididos en 5 batches de 10, el tiempo pasa de ~25 s (5 s por batch Ã— 5 batches) a ~5â€“7 s (latencia del batch mÃ¡s lento). Se envian en batches y no todos separados para agotar los lÃ­mites de la API de Groq.


### LLM: Groq + llama-3.3-70b-versatile

Se eligiÃ³ Groq por su velocidad y tier gratuito generoso.

### Streaming con SSE en lugar de una sola respuesta

Aunque los batches corren en paralelo en el servidor, el cliente sigue recibiendo resultados progresivos via SSE. Esto evita que la UI quede bloqueada mientras espera la respuesta final completa.

### Persistencia en localStorage

No se requiere backend ni base de datos. Los datos del CSV y los resultados del anÃ¡lisis se guardan directamente en el navegador, manteniendo la aplicaciÃ³n sin servidor adicional.

### Design tokens con CSS custom properties

Todo el sistema de color se define como variables CSS (`--accent`, `--ink-1..5`, `--surface`, `--elevated`, `--chart-1..8`). El cambio de tema claro/oscuro es instantÃ¡neo sin necesidad de un provider de React â€” se aplica con `data-theme="dark"` en el `<html>`.

### Arquitectura de componentes

- `ui/` contiene componentes reutilizables como Card, Badge, Modal, FilterSelect.
- Los directorios de dominio (`clients/`, `sellers/`, `insights/`) contienen componentes que sÃ­ tienen conocimiento del modelo de datos.
- Las pÃ¡ginas son Server Components por defecto; solo los componentes interactivos usan `"use client"`.
